model_name: "gpt2"
task_type: "causal_lm"
seed: 42
output_dir: "outputs/optimized_lm"

data:
  dataset_name: "wikitext"
  dataset_config_name: "wikitext-2-raw-v1"
  max_length: 128
  batch_size: 8
  num_workers: 0

train:
  lr: 5e-5
  weight_decay: 0.01
  num_epochs: 3
  grad_clip: 1.0
